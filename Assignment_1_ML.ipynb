{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 1 ML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pPZWp3ov3vJ"
      },
      "source": [
        "## Part 1: Text Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCRmrUCuyibv"
      },
      "source": [
        "In this part we are importing the movie reviews that we will be working with for this assignment. We are also importing all the modules that will be used in the rest of the code.\n",
        "We then proceed to handle the files, separate them in two lists, one in which we store the contents of the file, and one where we assign a value of -1 or 1 depending on whether the review was negative or positive, respectively.\n",
        "We also do a very basic text processing of the reviews, and we also split them into tokens, which will be saved in a list that will later be turned to a set to avoid duplicates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko1YHRcW41kv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddff6f7c-d35e-4397-bcb8-103ba000c35a"
      },
      "source": [
        "!wget -N http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import random"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-21 23:34:43--  http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘review_polarity.tar.gz’ not modified on server. Omitting download.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHD3ley8516y"
      },
      "source": [
        "files = !tar xvzf review_polarity.tar.gz\n",
        "docs, labels = [], []\n",
        "files = list(files)\n",
        "for file in files[:1000]:\n",
        "     with open(file) as f:\n",
        "         lines = f.read()\n",
        "         docs.append(lines)\n",
        "     labels.append(-1)\n",
        "for file in files[1000:2000]:\n",
        "     with open(file) as f:\n",
        "         lines = f.readline()\n",
        "         docs.append(lines)\n",
        "     labels.append(1)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzJaScEuAw1o"
      },
      "source": [
        "#TEXT PROCESSING\n",
        "newdocs = []\n",
        "vocab = []\n",
        "\n",
        "for doc in docs:\n",
        "  doc = doc.strip()\n",
        "  doc = doc.replace('\\n', '')\n",
        "  newdocs.append(doc)\n",
        "  doc = doc.split()\n",
        "  vocab.extend(doc)\n",
        "\n",
        "vocab = sorted(set(vocab))"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM5eB-G5mglb"
      },
      "source": [
        "featuredict = dict()\n",
        "for i, word in enumerate(vocab):\n",
        "  featuredict[word] = i"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_F1ortjjs_u"
      },
      "source": [
        "## Part 2: Feature extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onapINgbzlxh"
      },
      "source": [
        "This is the part that we basically do what the vectorizer would have done and create lists that we will use as our vectors. We then turn the \"vectors\" into numpy arrays and then assign them all to a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbz9pK-S8A_g"
      },
      "source": [
        "allvectors = []\n",
        "for doc in docs:\n",
        "  vector = []\n",
        "  for word in vocab:\n",
        "    if word in doc:\n",
        "      vector.append(1)\n",
        "    else:\n",
        "      vector.append(0)\n",
        "  vector = np.array(vector)\n",
        "  allvectors.append(vector)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y9pWyL1ju7P"
      },
      "source": [
        "## Part 3: Learning framework \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6-b-l_Iz-1s"
      },
      "source": [
        "For the convenience of having both training and testing data, we split our dataset here (80/20).\n",
        "\n",
        "This is the most important part that holds our class \"Model\" that simulates what sklearn would have done. \n",
        "\n",
        "We initialize the class with default values for the learning rate and the regulizer dampening factor and also a hyperplane (omega) that is set to None. \n",
        "\n",
        "By calling the fit method, we follow all the required mathematical computations for the Gradient Descent and the Hinge Loss, and we find the our best hyperplane. By calling the method score, we also call the method predict which will give us the predicted results by the model, that is -1 or 1 for each of the reviewed documents and it will eventually yield the accuracy of our model. The methods plot and weights, give us a plot and chart of all the highest weighting words respectively. \n",
        "\n",
        "Many of the words given by the weights method appear to be letters, which I attribute to the simple tokenization process that proceeded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bmDRv-8a3Tk"
      },
      "source": [
        "#Splitting the data\n",
        "X_train = allvectors[:800] + allvectors[1000:1800]\n",
        "X_test = allvectors[800:1000] + allvectors[1800:2000]\n",
        "y_train = labels[:800] + labels[1000:1800]\n",
        "y_test = labels[800:1000] + labels[1800:2000]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFrpQ7ccHpBJ"
      },
      "source": [
        "class Model:\n",
        "  def __init__(self, learning_rate = 0.001, regulizer_dampening = 0.001):\n",
        "    self.omega = None\n",
        "    self.learning_rate = learning_rate #gamma\n",
        "    self.regulizer_dampening = regulizer_dampening #lambda\n",
        "  \n",
        "  def fit(self, X, y):\n",
        "    X = np.c_[np.ones((len(X),1)),X]\n",
        "    self.omega = np.random.normal(0, 1, size=len(X[0]))\n",
        "    omega_norm = norm(self.omega)**2\n",
        "    sum = 0\n",
        "    for i, array in enumerate(X):\n",
        "      sum += max(0,1 - y[i] * np.vdot(self.omega, array))\n",
        "    hingeloss = (self.regulizer_dampening/2)* omega_norm + sum\n",
        "    \n",
        "    #GRADIENT\n",
        "    sum2 = np.zeros(len(X[0]))\n",
        "    for i, array in enumerate(X):\n",
        "      if y[i] * np.vdot(self.omega, array) < 1:\n",
        "        sum2 += -(y[i] * array)\n",
        "    gradient = self.regulizer_dampening * self.omega + sum2     \n",
        "    self.omega = self.omega - self.learning_rate * gradient\n",
        "    omega_norm = norm(self.omega)**2\n",
        "    sum = 0\n",
        "    for i, array in enumerate(X):\n",
        "     sum += max(0,1 - y[i] * np.vdot(self.omega, array))\n",
        "    hingeloss = (self.regulizer_dampening/2)* omega_norm + sum\n",
        "\n",
        "    #HINGE LOSS\n",
        "    besthingeloss = hingeloss\n",
        "    counter = 0\n",
        "    for i in range(1000):\n",
        "      sum2 = np.zeros(len(X[0]))\n",
        "      for i, array in enumerate(X):\n",
        "        if y[i] * np.vdot(self.omega, array) < 1:\n",
        "          sum2 += -(y[i] * array)\n",
        "        gradient = self.regulizer_dampening * self.omega + sum2 #GRADIENT DESCENT\n",
        "        self.omega = self.omega - self.learning_rate * gradient #OMEGA\n",
        "      sum = 0\n",
        "      for i, array in enumerate(X):\n",
        "        sum += max(0,1 - y[i] * np.vdot(self.omega, array))\n",
        "      hingeloss = (self.regulizer_dampening/2)* norm(self.omega) + sum  \n",
        "      if hingeloss > besthingeloss - 0.001:\n",
        "        counter +=1 \n",
        "      if hingeloss > besthingeloss - 0.001 and counter == 5:\n",
        "        hingeloss = besthingeloss\n",
        "        break\n",
        "      if hingeloss < besthingeloss:\n",
        "        besthingeloss = hingeloss\n",
        "\n",
        "  def predict(self, X, y):\n",
        "    X = np.c_[np.ones((len(X),1)),X]\n",
        "    predicted = []\n",
        "    for vector in X:\n",
        "      result = np.matmul(self.omega, vector)\n",
        "      signfunction = np.sign(result)\n",
        "      predicted.append(signfunction)\n",
        "    return predicted\n",
        "\n",
        "  def score(self, X, y):\n",
        "    predicted = self.predict(X,y)\n",
        "    counter = 0\n",
        "    for i, element in enumerate(y):\n",
        "      if element == predicted[i].item(0):\n",
        "        counter += 1\n",
        "    return counter/len(y)*100\n",
        "  \n",
        "  def plot(self):\n",
        "    import matplotlib.pyplot as plt\n",
        "    x = np.arange(0, len(self.omega.T), 1)\n",
        "    plt.figure(figsize=(20, 3))\n",
        "    plt.plot(x[1:], self.omega[1:])\n",
        "    plt.xlabel(\"Value\")\n",
        "    plt.ylabel(\"Weights\")\n",
        "    plt.show()\n",
        "\n",
        "  def weights(self, X_raw, vocab):\n",
        "    idx = np.argsort(np.abs(self.omega[1:]))\n",
        "    print(\"                Word   Weight  Occurences\")\n",
        "    for i in idx[-20:]:  \n",
        "      print(\"%20s   %.3f\\t%i \" % (vocab[i], self.omega[i+1], np.sum([vocab[i] in d for d in X_raw])))"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT2q2DdGM9nc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49bc9a90-3bc7-4294-cbda-32cf257a48d6"
      },
      "source": [
        "#model = Model()\n",
        "#model.fit(X_train, y_train)\n",
        "#model.score(X_train, y_train)\n",
        "#model.weights(docs, vocab)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                Word   Weight  Occurences\n",
            "                more   -95.070\t781 \n",
            "                this   -95.533\t1079 \n",
            "                   p   99.781\t1703 \n",
            "                   d   100.010\t1888 \n",
            "                   .   102.990\t1905 \n",
            "                  er   106.225\t1711 \n",
            "                   m   106.797\t1908 \n",
            "                   l   107.111\t1921 \n",
            "                  he   109.114\t1742 \n",
            "                   h   111.926\t1949 \n",
            "                   b   114.700\t1715 \n",
            "                   r   116.271\t1962 \n",
            "                   n   116.559\t1964 \n",
            "                   s   118.587\t1979 \n",
            "                   t   119.846\t1984 \n",
            "                   a   120.223\t1984 \n",
            "                   o   120.262\t1983 \n",
            "                   i   121.449\t1988 \n",
            "                   e   122.058\t1990 \n",
            "                   c   131.487\t1844 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSlwPrX1wVCq"
      },
      "source": [
        "## Part 4: Exploring hyperparameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUsokY-M6M5E"
      },
      "source": [
        "For the final part of the code, we iterate through 10 different combinations of hyperparameters and we check to see which one performed the best. We then take these specific ones and use them to apply our model to the test data that we have reserved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI4m1LswmL1o",
        "outputId": "34d8e4c8-7cdc-4bc6-9992-0ad5553bef84"
      },
      "source": [
        "best_hyperparameters = None\n",
        "grid = {'learning_rate': np.exp(np.linspace(np.log(0.0001), np.log(3), 10)),\n",
        "        'reguliser_dampening': np.exp(np.linspace(np.log(0.0001), np.log(3), 10))}\n",
        "\n",
        "print(\"Learning rate:\\tReg.dampening:\\tTraining set accuracy:\")\n",
        "\n",
        "for i in range(10):\n",
        "  lr = grid['learning_rate'][random.randint(0,9)]\n",
        "  rd = grid['reguliser_dampening'][random.randint(0,9)]\n",
        "  model = Model(learning_rate = lr, regulizer_dampening = rd )\n",
        "  model.fit(X_train, y_train)\n",
        "  training_accuracy = model.score(X_train, y_train)\n",
        "  if best_hyperparameters is None or best_hyperparameters[2] < training_accuracy:\n",
        "    best_hyperparameters = (lr, rd, training_accuracy)\n",
        "  print(\"%.5f\\t\\t%.5f\\t\\t%.1f%%\" % (lr, rd, training_accuracy))\n",
        "\n",
        "best_learning_rate = best_hyperparameters[0]\n",
        "best_reguliser_dampening = best_hyperparameters[1]\n",
        "print(\"Best parameters: %.5f, %.5f\" % (best_learning_rate, best_reguliser_dampening))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:\tReg.dampening:\tTraining set accuracy:\n",
            "0.00311\t\t0.00031\t\t99.5%\n",
            "0.00031\t\t0.00099\t\t100.0%\n",
            "0.00311\t\t0.00311\t\t99.8%\n",
            "0.00099\t\t0.09655\t\t80.2%\n",
            "0.30353\t\t0.00311\t\t89.8%\n",
            "0.95425\t\t0.00311\t\t97.4%\n",
            "0.00977\t\t0.00031\t\t100.0%\n",
            "0.09655\t\t0.95425\t\t60.8%\n",
            "0.09655\t\t3.00000\t\t50.1%\n",
            "0.00099\t\t0.03071\t\t99.5%\n",
            "Best parameters: 0.00031, 0.00099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwuRrM8HcOrv",
        "outputId": "f82d41a5-af03-49cb-c4f0-e29a577c41a7"
      },
      "source": [
        "model = Model(learning_rate = best_learning_rate, regulizer_dampening = best_reguliser_dampening)\n",
        "model.fit(X_train, y_train)\n",
        "acc = model.score(X_test, y_test)\n",
        "print(\"Test Accuracy:\", acc)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 99.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVYb47TVBXSW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i11qurve_6pB"
      },
      "source": [
        "# Qualitative Analysis\n",
        "\n",
        "With the convergence of the hinge loss and the creation of the hyperplane, we obtained high accuracy scores for both the training and the test set.\n",
        "By looking at the weights for the words with the highest 'voting' values, it is easy to understand that something went wrong with the tokenization of the documents most probably. It is not very sound to encounter single letters that have such positive weights. However, there are still 2 full words, \"more\" and \"this\", which are associated with around -95. This actually makes sense for \"more\", since if it were to be used in a movie review it would call for \"more\" of something, rendering the movie less than what it should have been.\n",
        "\n",
        "The hyperparameters that were found to be the best (for one instance) were 0.00031 and 0.00099, which can denote that a smaller but steadier learning rate and regulizer dampening are more suitable for our model. Having the initial learning rate of 0.1 resulted in a hinge loss 100x bigger than the first hinge loss produced, and lowering the learning rate gave a better balanced outcome.\n",
        "\n",
        "In terms of design, the choice of a class for implementing the model was ideal, as it can withhold information and it is easier to access class methods. In terms of data structures, I made use of arrays, lists and dictionaries, which facilitated different functions each. Arrays were extremely helpful for indexing and accessing specific elements or rows/columns, as well as for mathematical operations which were heavily used througout the assignment.\n",
        "One of the choices I made that I think I should comment on is the choice of a for loop for 1000 iterations for the search of the best hinge loss instead of a while loop, which was based on the idea of a more efficient code that takes less time in the case that the best hinge loss is not found.\n",
        "\n",
        "# Comments\n",
        "\n",
        "In general, the implementation of the whole assignment was a tough challenge for people with minimal math knowledge, and trying to code the formulas without really understanding the theory behind them proved to work but it definitely left us greatly confused. However it was indeed educational since we got to understand what the backbone of a Machine Learning model looks like and provided us with deep insight into its workings. Part 3 was definitely the one that required the most time out of all the assignment and also was the hardest to implement.\n"
      ]
    }
  ]
}